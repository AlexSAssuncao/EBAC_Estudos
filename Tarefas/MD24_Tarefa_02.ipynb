{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Diferenças entre AdaBoost e Gradient Boosting Machine (GBM)\n",
    "\n",
    "1.  **Método de Otimização:**\n",
    "    * **AdaBoost:** Atualiza os pesos das amostras com base nos erros de classificação, focando em amostras mal classificadas.\n",
    "    * **GBM:** Usa gradiente descendente para minimizar uma função de perda, ajustando os modelos subsequentes para corrigir os erros residuais (gradientes) do modelo anterior.\n",
    "\n",
    "2.  **Função de Perda:**\n",
    "    * **AdaBoost:** Usa uma função de perda exponencial, que dá mais peso a amostras mal classificadas.\n",
    "    * **GBM:** Permite o uso de várias funções de perda, como erro quadrático médio (para regressão) ou log loss (para classificação), oferecendo maior flexibilidade.\n",
    "\n",
    "3.  **Modelos Base:**\n",
    "    * **AdaBoost:** Tipicamente usa árvores de decisão rasas (stumps) como modelos base.\n",
    "    * **GBM:** Também usa árvores de decisão, mas pode usar árvores mais profundas e complexas, permitindo capturar padrões mais intrincados nos dados.\n",
    "\n",
    "4.  **Atualização dos Modelos:**\n",
    "    * **AdaBoost:** Atualiza os pesos das amostras e os pesos dos modelos fracos.\n",
    "    * **GBM:** Ajusta os modelos subsequentes para prever os erros residuais (gradientes) do modelo anterior, sem alterar os pesos das amostras.\n",
    "\n",
    "5.  **Robustez a Outliers:**\n",
    "    * **AdaBoost:** Pode ser sensível a outliers, pois eles podem receber pesos elevados e influenciar significativamente os modelos subsequentes.\n",
    "    * **GBM:** Geralmente é mais robusto a outliers, pois a função de perda pode ser ajustada para minimizar o impacto de valores extremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gera um data set\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treina um GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43848663277068134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treina um GradientBoostingRegressor\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test[1:2])\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 Hiperparâmetros Importantes do GradientBoostingClassifier e GradientBoostingRegressor\n",
    "\n",
    "\n",
    "1.  **`n_estimators=100`**:\n",
    "    * **Função:** Número de estágios de boosting (número de árvores a serem construídas).\n",
    "    * **Explicação:** Controla a complexidade do modelo. Aumentar `n_estimators` pode melhorar a precisão, mas também aumentar o tempo de treinamento e o risco de overfitting. É crucial ajustar este parâmetro em conjunto com `learning_rate`.\n",
    "\n",
    "2.  **`learning_rate=0.1`**:\n",
    "    * **Função:** Contribuição de cada árvore para o resultado final.\n",
    "    * **Explicação:** Controla a taxa de aprendizado. Valores menores de `learning_rate` requerem mais árvores (`n_estimators`) para alcançar um bom desempenho, mas podem melhorar a generalização. É comum ajustar `learning_rate` e `n_estimators` em conjunto.\n",
    "\n",
    "3.  **`max_depth=3`**:\n",
    "    * **Função:** Profundidade máxima de cada árvore de decisão.\n",
    "    * **Explicação:** Controla a complexidade das árvores. Valores maiores de `max_depth` permitem que as árvores capturem padrões mais complexos, mas também aumentam o risco de overfitting. É comum usar valores baixos para `max_depth` (por exemplo, 3-5).\n",
    "\n",
    "4.  **`min_samples_split=2`**:\n",
    "    * **Função:** Número mínimo de amostras necessárias para dividir um nó interno.\n",
    "    * **Explicação:** Controla a complexidade das árvores. Aumentar `min_samples_split` restringe a criação de nós com poucas amostras, o que pode ajudar a evitar o overfitting.\n",
    "\n",
    "5.  **`subsample=1.0`**:\n",
    "    * **Função:** Fração de amostras a serem usadas para treinar cada árvore.\n",
    "    * **Explicação:** Controla a estocasticidade do boosting. Valores menores de `subsample` introduzem aleatoriedade, o que pode ajudar a reduzir a variância e evitar o overfitting. Usar um valor menor que 1.0 cria um modelo chamado Stochastic Gradient Boosting.\n",
    "\n",
    "```python\n",
    "GradientBoostingClassifier(\n",
    "    loss='log_loss', \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100, \n",
    "    ubsample=1.0, \n",
    "    criterion='friedman_mse', \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_depth=3, \n",
    "    min_impurity_decrease=0.0, \n",
    "    init=None, \n",
    "    andom_state=None, \n",
    "    max_features=None, \n",
    "    verbose=0, \n",
    "    max_leaf_nodes=None, \n",
    "    warm_start=False, \n",
    "    validation_fraction=0.1, \n",
    "    n_iter_no_change=None, \n",
    "    tol=0.0001, \n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "\n",
    "GradientBoostingRegressor(\n",
    "    loss='squared_error', \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100, \n",
    "    subsample=1.0, \n",
    "    riterion='friedman_mse', \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_depth=3, \n",
    "    min_impurity_decrease=0.0, \n",
    "    init=None, \n",
    "    random_state=None, \n",
    "    max_features=None, \n",
    "    alpha=0.9, \n",
    "    verbose=0, \n",
    "    max_leaf_nodes=None, \n",
    "    warm_start=False, \n",
    "    validation_fraction=0.1, \n",
    "    n_iter_no_change=None, \n",
    "    tol=0.0001, \n",
    "    ccp_alpha=0.0\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - GridSearch para GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gera um data set\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(learning_rate=1, n_estimators=550, random_state=42)\n",
      "0.9224\n"
     ]
    }
   ],
   "source": [
    "# fazendo o grid search\n",
    "rf = GradientBoostingClassifier()\n",
    "params = {\n",
    "    'n_estimators': list(range(50, 1001, 50)),\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(estimator = rf,\n",
    "                        param_grid = params,\n",
    "                        scoring = 'accuracy',\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "#print os melhores parametros\n",
    "print(grid_rf.best_estimator_)\n",
    "print(grid_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stochastic Gradient Boosting e Gradient Boosting\n",
    "\n",
    "* **Gradient Boosting:**\n",
    "    * Usa todo o conjunto de dados para treinar cada árvore sequencialmente.\n",
    "    * Pode ser mais propenso a overfitting em conjuntos de dados grandes.\n",
    "\n",
    "* **Stochastic Gradient Boosting:**\n",
    "    * Usa uma amostra aleatória do conjunto de dados para treinar cada árvore.\n",
    "    * Introduz aleatoriedade, o que ajuda a reduzir o overfitting e pode acelerar o treinamento.\n",
    "    * É uma versão mais robusta e eficiente do Gradient Boosting.\n",
    "\n",
    "Em resumo, o Stochastic Gradient Boosting é uma variação do Gradient Boosting que adiciona aleatoriedade ao processo de treinamento, tornando-o mais robusto e eficiente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
